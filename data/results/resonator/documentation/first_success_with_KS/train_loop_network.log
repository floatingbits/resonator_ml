[2026-01-21 10:05:31] Config(results_path='data/results', resonator_results_sub_path='resonator/workspace', resonator_training_path='data/processed/decay_only', cache_path='data/cache', loop_filer_training_data_cache_sub_path='loop_filter_training_data', instrument_name='KS_Short_E', training_parameters_version='v2.4', sample_rate=44100, base_frequency=82.46, training_parameters=TrainingParameters(batch_size=32, epochs=100, learning_rate=8e-06, loss_function=<function relative_l1_with_penalty at 0x7489ba56ba60>, max_training_data_frames=20000), neural_network_parameters=NeuralNetworkParameters(num_hidden_per_layer=20, num_hidden_layers=1, delay_patterns=[DelayPattern(t_factor=0, n_before=0, n_after=2), DelayPattern(t_factor=1, n_before=3, n_after=2), DelayPattern(t_factor=0.25, n_before=2, n_after=2), DelayPattern(t_factor=0.75, n_before=2, n_after=2), DelayPattern(t_factor=0.5, n_before=2, n_after=2)], activation=Tanh()))
[2026-01-21 10:05:31] TrainingParameters(batch_size=32, epochs=100, learning_rate=8e-06, loss_function=<function relative_l1_with_penalty at 0x7489ba56ba60>, max_training_data_frames=20000)
[2026-01-21 10:05:31] NeuralNetworkModule(
[2026-01-21 10:05:31]   (net): Sequential(
[2026-01-21 10:05:31]     (0): Linear(in_features=24, out_features=20, bias=True)
[2026-01-21 10:05:31]     (1): Tanh()
[2026-01-21 10:05:31]     (2): Linear(in_features=20, out_features=1, bias=True)
[2026-01-21 10:05:31]   )
[2026-01-21 10:05:31] )
[2026-01-21 10:05:31] Loading dataset from cache
[2026-01-21 10:05:31] Dataloader time  0.0031821727752685547 seconds!
[2026-01-21 10:05:37] Epoch 1/100  Loss: 242.0698593084
[2026-01-21 10:05:38] Epoch 2/100  Loss: 97.0551820172
[2026-01-21 10:05:38] Epoch 3/100  Loss: 4.3565482395
[2026-01-21 10:05:39] Epoch 4/100  Loss: 1.6622515217
[2026-01-21 10:05:40] Epoch 5/100  Loss: 1.2246282741
[2026-01-21 10:05:41] Epoch 6/100  Loss: 0.9956246303
[2026-01-21 10:05:41] Epoch 7/100  Loss: 0.8678531549
[2026-01-21 10:05:42] Epoch 8/100  Loss: 0.7532027337
[2026-01-21 10:05:43] Epoch 9/100  Loss: 0.6461133527
[2026-01-21 10:05:43] Epoch 10/100  Loss: 0.5404463941
[2026-01-21 10:05:44] Epoch 11/100  Loss: 0.4473836397
[2026-01-21 10:05:45] Epoch 12/100  Loss: 0.3487764188
[2026-01-21 10:05:46] Epoch 13/100  Loss: 0.2625125593
[2026-01-21 10:05:46] Epoch 14/100  Loss: 0.1831579493
[2026-01-21 10:05:47] Epoch 15/100  Loss: 0.1324470108
[2026-01-21 10:05:48] Epoch 16/100  Loss: 0.1133421450
[2026-01-21 10:05:48] Epoch 17/100  Loss: 0.1018693792
[2026-01-21 10:05:49] Epoch 18/100  Loss: 0.0974888736
[2026-01-21 10:05:50] Epoch 19/100  Loss: 0.0899382499
[2026-01-21 10:05:51] Epoch 20/100  Loss: 0.0864747381
[2026-01-21 10:05:51] Epoch 21/100  Loss: 0.0799020751
[2026-01-21 10:05:52] Epoch 22/100  Loss: 0.0768967789
[2026-01-21 10:05:53] Epoch 23/100  Loss: 0.0728478399
[2026-01-21 10:05:54] Epoch 24/100  Loss: 0.0699194261
[2026-01-21 10:05:54] Epoch 25/100  Loss: 0.0676712875
[2026-01-21 10:05:55] Epoch 26/100  Loss: 0.0662766995
[2026-01-21 10:05:56] Epoch 27/100  Loss: 0.0623774831
[2026-01-21 10:05:56] Epoch 28/100  Loss: 0.0583342495
[2026-01-21 10:05:57] Epoch 29/100  Loss: 0.0592695380
[2026-01-21 10:05:58] Epoch 30/100  Loss: 0.0538326424
[2026-01-21 10:05:59] Epoch 31/100  Loss: 0.0521928004
[2026-01-21 10:05:59] Epoch 32/100  Loss: 0.0498166125
[2026-01-21 10:06:00] Epoch 33/100  Loss: 0.0491259613
[2026-01-21 10:06:01] Epoch 34/100  Loss: 0.0456379109
[2026-01-21 10:06:02] Epoch 35/100  Loss: 0.0463633068
[2026-01-21 10:06:02] Epoch 36/100  Loss: 0.0426954599
[2026-01-21 10:06:03] Epoch 37/100  Loss: 0.0432443426
[2026-01-21 10:06:04] Epoch 38/100  Loss: 0.0442190264
[2026-01-21 10:06:04] Epoch 39/100  Loss: 0.0415538623
[2026-01-21 10:06:05] Epoch 40/100  Loss: 0.0414607238
[2026-01-21 10:06:06] Epoch 41/100  Loss: 0.0421134608
[2026-01-21 10:06:07] Epoch 42/100  Loss: 0.0395492546
[2026-01-21 10:06:07] Epoch 43/100  Loss: 0.0399619255
[2026-01-21 10:06:08] Epoch 44/100  Loss: 0.0386107105
[2026-01-21 10:06:09] Epoch 45/100  Loss: 0.0379871182
[2026-01-21 10:06:10] Epoch 46/100  Loss: 0.0378352698
[2026-01-21 10:06:10] Epoch 47/100  Loss: 0.0432032575
[2026-01-21 10:06:11] Epoch 48/100  Loss: 0.0367754748
[2026-01-21 10:06:12] Epoch 49/100  Loss: 0.0347707897
[2026-01-21 10:06:13] Epoch 50/100  Loss: 0.0356189142
[2026-01-21 10:06:13] Epoch 51/100  Loss: 0.0357956775
[2026-01-21 10:06:14] Epoch 52/100  Loss: 0.0349652824
[2026-01-21 10:06:15] Epoch 53/100  Loss: 0.0355993606
[2026-01-21 10:06:15] Epoch 54/100  Loss: 0.0335437293
[2026-01-21 10:06:16] Epoch 55/100  Loss: 0.0334672783
[2026-01-21 10:06:17] Epoch 56/100  Loss: 0.0342036046
[2026-01-21 10:06:18] Epoch 57/100  Loss: 0.0341688946
[2026-01-21 10:06:18] Epoch 58/100  Loss: 0.0334548088
[2026-01-21 10:06:19] Epoch 59/100  Loss: 0.0318349377
[2026-01-21 10:06:20] Epoch 60/100  Loss: 0.0327481693
[2026-01-21 10:06:20] Epoch 61/100  Loss: 0.0325649807
[2026-01-21 10:06:21] Epoch 62/100  Loss: 0.0320899714
[2026-01-21 10:06:22] Epoch 63/100  Loss: 0.0316955720
[2026-01-21 10:06:23] Epoch 64/100  Loss: 0.0320875535
[2026-01-21 10:06:23] Epoch 65/100  Loss: 0.0281304408
[2026-01-21 10:06:24] Epoch 66/100  Loss: 0.0303997620
[2026-01-21 10:06:25] Epoch 67/100  Loss: 0.0301031078
[2026-01-21 10:06:26] Epoch 68/100  Loss: 0.0281343904
[2026-01-21 10:06:27] Epoch 69/100  Loss: 0.0288175843
[2026-01-21 10:06:27] Epoch 70/100  Loss: 0.0297072314
[2026-01-21 10:06:28] Epoch 71/100  Loss: 0.0285648322
[2026-01-21 10:06:29] Epoch 72/100  Loss: 0.0282968687
[2026-01-21 10:06:30] Epoch 73/100  Loss: 0.0273103089
[2026-01-21 10:06:31] Epoch 74/100  Loss: 0.0274963778
[2026-01-21 10:06:31] Epoch 75/100  Loss: 0.0264425218
[2026-01-21 10:06:32] Epoch 76/100  Loss: 0.0267658006
[2026-01-21 10:06:33] Epoch 77/100  Loss: 0.0250734980
[2026-01-21 10:06:34] Epoch 78/100  Loss: 0.0262958768
[2026-01-21 10:06:34] Epoch 79/100  Loss: 0.0243897364
[2026-01-21 10:06:35] Epoch 80/100  Loss: 0.0251025962
[2026-01-21 10:06:36] Epoch 81/100  Loss: 0.0233555291
[2026-01-21 10:06:37] Epoch 82/100  Loss: 0.0256911360
[2026-01-21 10:06:37] Epoch 83/100  Loss: 0.0233680321
[2026-01-21 10:06:38] Epoch 84/100  Loss: 0.0236830412
[2026-01-21 10:06:39] Epoch 85/100  Loss: 0.0231209380
[2026-01-21 10:06:40] Epoch 86/100  Loss: 0.0219597376
[2026-01-21 10:06:40] Epoch 87/100  Loss: 0.0219271210
[2026-01-21 10:06:41] Epoch 88/100  Loss: 0.0216179973
[2026-01-21 10:06:42] Epoch 89/100  Loss: 0.0216086574
[2026-01-21 10:06:43] Epoch 90/100  Loss: 0.0216166854
[2026-01-21 10:06:44] Epoch 91/100  Loss: 0.0202730960
[2026-01-21 10:06:45] Epoch 92/100  Loss: 0.0238985518
[2026-01-21 10:06:46] Epoch 93/100  Loss: 0.0202827091
[2026-01-21 10:06:46] Epoch 94/100  Loss: 0.0209311554
[2026-01-21 10:06:47] Epoch 95/100  Loss: 0.0192426724
[2026-01-21 10:06:48] Epoch 96/100  Loss: 0.0199921983
[2026-01-21 10:06:49] Epoch 97/100  Loss: 0.0188491040
[2026-01-21 10:06:49] Epoch 98/100  Loss: 0.0201141821
[2026-01-21 10:06:50] Epoch 99/100  Loss: 0.0202317752
[2026-01-21 10:06:51] Epoch 100/100  Loss: 0.0193495654
[2026-01-21 10:06:51] Training abgeschlossen.
[2026-01-21 10:06:51] It took 79.78637647628784 seconds!
